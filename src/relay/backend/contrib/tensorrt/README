# Relay TensorRT Integration

Currently, Relay/TRT integration only works when the entire model can be converted to TRT. It is enabled with the `EnableTrt` pass. If any op in the model cannot be converted to TRT, `EnableTrt` will return the original module unmodified.

# How to use
1. Build TVM with cmake flag `USE_TENSORRT=ON` or `USE_TENSORRT=/path/to/TensorRT`. USE_CUDA should be enabled as well.

2. Convert the model into TensorRT. This step will determine if every node in the graph can be converted to TensorrRT and if so will mark the graph to use TensorRT and apply some specific optimization passes.
```python
import tvm.relay.tensorrt
mod = relay.tensorrt.EnableTrt(mod, params)
```

3. Check if TRT was enabled. If not, it means some op in the graph is not supported by the TensorRT conversion. EnableTrt will output which particular ops are not supported and why.
```python
assert mod['main'].attrs and mod['main'].attrs.Compiler == 'tensorrt'
```

4. Finish compilation.
```python
with relay.build_config(opt_level=2, disabled_pass={"SimplifyInference"}):
  graph, lib, params = relay.build(mod, "cuda", params=params)
```

5. (Optional) Serialize/deserialize the compiled model. The model will be serialized to three files: `compiled.json`, `compiled.params`, and `compiled.tensorrt`.
```python
# Serialize
with open('compiled.json', 'w') as f_graph_json:
  f_graph_json.write(graph)
with open('compiled.params', 'wb') as f_params:
  f_params.write(relay.save_param_dict(params))
lib.save('compiled.tensorrt')

# Deserialize
with open('compiled.json', 'r') as f_graph_json:
  graph = f_graph_json.read()
with open('compiled.params', 'rb') as f_params:
  params = tvm.relay.load_param_dict(f_params.read())
lib = tvm.module.load("compiled.tensorrt")
```

6. Run inference. The first invocation will trigger creation of the TensorRT engine. This could take up to a few minutes.
```python
# Create graph runtime
mod = graph_runtime.create(graph, lib, ctx=tvm.gpu(0))
mod.set_input(**params)

i_data = np.random.uniform(0, 1, input_shape).astype(dtype)
# Build TensorRT engine
mod.run(data=i_data)

# Run inference
mod.run(data=i_data)
res = mod.get_output(0)
```



The tests `tests/python/relay/test_tensorrt.py` provide some deeper examples of how to use this feature.

The NNVM/TRT integration is still present.